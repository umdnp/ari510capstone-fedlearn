{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison Framework\n",
    "\n",
    "This notebook compares multiple classification algorithms for predicting prolonged ICU stays.\n",
    "\n",
    "**Objectives:**\n",
    "- Establish baseline performance across multiple classifiers\n",
    "- Identify best-performing algorithms for this dataset\n",
    "- Test SGDClassifier (required for partial fitting in federated learning)\n",
    "- Compare results to Jim's logistic regression baseline (Accuracy: 70.71%, F1: 65.84%)\n",
    "\n",
    "**Models to Test:**\n",
    "1. Logistic Regression (baseline)\n",
    "2. Random Forest\n",
    "3. Gradient Boosting\n",
    "4. SGDClassifier (supports partial_fit for federated learning)\n",
    "5. Support Vector Machine (SVM)\n",
    "6. Decision Tree\n",
    "7. K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../src')\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Evaluation\n",
    "from fedlearn.evaluation import evaluate_model\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the cleaned feature set from `v_features_icu_stay_clean` view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (199646, 77)\n",
      "Target distribution:\n",
      "prolonged_stay\n",
      "0    150206\n",
      "1     49440\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class balance: prolonged_stay\n",
      "0    0.7524\n",
      "1    0.2476\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Connect to database\n",
    "db_path = Path(\"../../data/duckdb/fedlearn.duckdb\")\n",
    "conn = duckdb.connect(str(db_path), read_only=True)\n",
    "\n",
    "# Load data\n",
    "df = conn.execute(\"SELECT * FROM v_features_icu_stay_clean\").df()\n",
    "conn.close()\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(df['prolonged_stay'].value_counts())\n",
    "print(f\"\\nClass balance: {df['prolonged_stay'].value_counts(normalize=True).round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Features and Target\n",
    "\n",
    "Using the same approach as Jim's baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 73\n",
      "Numeric features: 64\n",
      "Categorical features: 9\n"
     ]
    }
   ],
   "source": [
    "# Target variable\n",
    "y = df[\"prolonged_stay\"]\n",
    "\n",
    "# Features (drop ID, target, and high-cardinality text field)\n",
    "X = df.drop(\n",
    "    columns=[\"patientunitstayid\", \"los_days\", \"prolonged_stay\", \"apacheadmissiondx\"]\n",
    ")\n",
    "\n",
    "# Define categorical and numeric features\n",
    "categorical_features = [\n",
    "    \"gender\",\n",
    "    \"ethnicity\",\n",
    "    \"unittype\",\n",
    "    \"unitadmitsource\",\n",
    "    \"hospitaladmitsource\",\n",
    "    \"admissiondx_category\",\n",
    "    \"numbedscategory\",\n",
    "    \"teachingstatus\",\n",
    "    \"hospital_region\",\n",
    "]\n",
    "\n",
    "numeric_features = [c for c in X.columns if c not in categorical_features]\n",
    "\n",
    "print(f\"Total features: {len(X.columns)}\")\n",
    "print(f\"Numeric features: {len(numeric_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "\n",
    "80/20 split with stratification to maintain class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 159,716\n",
      "Test set size: 39,930\n",
      "\n",
      "Training set class distribution:\n",
      "prolonged_stay\n",
      "0    120164\n",
      "1     39552\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set class distribution:\n",
      "prolonged_stay\n",
      "0    30042\n",
      "1     9888\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train):,}\")\n",
    "print(f\"Test set size: {len(X_test):,}\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n",
    "\n",
    "Reusing Jim's preprocessing approach:\n",
    "- **Numeric features**: Median imputation + StandardScaler\n",
    "- **Categorical features**: Most frequent imputation + One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing pipeline configured successfully\n"
     ]
    }
   ],
   "source": [
    "# Numeric preprocessing\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Categorical preprocessing\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combined preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Preprocessing pipeline configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models to Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured 7 models for comparison:\n",
      "  - Logistic Regression\n",
      "  - SGDClassifier\n",
      "  - Random Forest\n",
      "  - Gradient Boosting\n",
      "  - Decision Tree\n",
      "  - SVM (RBF)\n",
      "  - K-Nearest Neighbors\n"
     ]
    }
   ],
   "source": [
    "# Define all models to test\n",
    "models = {\n",
    "    \"Logistic Regression\": Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocess\", preprocessor),\n",
    "            (\n",
    "                \"clf\",\n",
    "                LogisticRegression(\n",
    "                    max_iter=500,\n",
    "                    n_jobs=-1,\n",
    "                    class_weight=\"balanced\",\n",
    "                    solver=\"lbfgs\",\n",
    "                    random_state=42,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    \"SGDClassifier\": Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocess\", preprocessor),\n",
    "            (\n",
    "                \"clf\",\n",
    "                SGDClassifier(\n",
    "                    loss=\"log_loss\",  # For logistic regression\n",
    "                    max_iter=1000,\n",
    "                    tol=1e-3,\n",
    "                    class_weight=\"balanced\",\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    \"Random Forest\": Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocess\", preprocessor),\n",
    "            (\n",
    "                \"clf\",\n",
    "                RandomForestClassifier(\n",
    "                    n_estimators=100,\n",
    "                    max_depth=10,\n",
    "                    min_samples_split=20,\n",
    "                    min_samples_leaf=10,\n",
    "                    class_weight=\"balanced\",\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    \"Gradient Boosting\": Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocess\", preprocessor),\n",
    "            (\n",
    "                \"clf\",\n",
    "                GradientBoostingClassifier(\n",
    "                    n_estimators=100,\n",
    "                    learning_rate=0.1,\n",
    "                    max_depth=5,\n",
    "                    min_samples_split=20,\n",
    "                    min_samples_leaf=10,\n",
    "                    random_state=42,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    \"Decision Tree\": Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocess\", preprocessor),\n",
    "            (\n",
    "                \"clf\",\n",
    "                DecisionTreeClassifier(\n",
    "                    max_depth=10,\n",
    "                    min_samples_split=20,\n",
    "                    min_samples_leaf=10,\n",
    "                    class_weight=\"balanced\",\n",
    "                    random_state=42,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    \"SVM (RBF)\": Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocess\", preprocessor),\n",
    "            (\n",
    "                \"clf\",\n",
    "                SVC(\n",
    "                    kernel=\"rbf\",\n",
    "                    C=1.0,\n",
    "                    class_weight=\"balanced\",\n",
    "                    random_state=42,\n",
    "                    probability=True,  # Enable probability estimates for ROC-AUC\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    \"K-Nearest Neighbors\": Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocess\", preprocessor),\n",
    "            (\n",
    "                \"clf\",\n",
    "                KNeighborsClassifier(\n",
    "                    n_neighbors=15,\n",
    "                    weights=\"distance\",\n",
    "                    n_jobs=-1,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(f\"Configured {len(models)} models for comparison:\")\n",
    "for name in models.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate All Models\n",
    "\n",
    "This may take several minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training and evaluation...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Training Logistic Regression...\n",
      "Logistic Regression\n",
      "----------------------------------------\n",
      "Accuracy  : 0.7080\n",
      "Precision : 0.6554\n",
      "Recall    : 0.6960\n",
      "F1-score  : 0.6602\n",
      "\n",
      "ROC-AUC   : 0.7620\n",
      "Training time: 6.93s\n",
      "================================================================================\n",
      "\n",
      "Training SGDClassifier...\n",
      "SGDClassifier\n",
      "----------------------------------------\n",
      "Accuracy  : 0.7042\n",
      "Precision : 0.6516\n",
      "Recall    : 0.6913\n",
      "F1-score  : 0.6559\n",
      "\n",
      "ROC-AUC   : 0.7546\n",
      "Training time: 4.73s\n",
      "================================================================================\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest\n",
      "----------------------------------------\n",
      "Accuracy  : 0.7299\n",
      "Precision : 0.6668\n",
      "Recall    : 0.7014\n",
      "F1-score  : 0.6751\n",
      "\n",
      "ROC-AUC   : 0.7764\n",
      "Training time: 6.76s\n",
      "================================================================================\n",
      "\n",
      "Training Gradient Boosting...\n",
      "Gradient Boosting\n",
      "----------------------------------------\n",
      "Accuracy  : 0.7944\n",
      "Precision : 0.7355\n",
      "Recall    : 0.6486\n",
      "F1-score  : 0.6686\n",
      "\n",
      "ROC-AUC   : 0.7973\n",
      "Training time: 170.55s\n",
      "================================================================================\n",
      "\n",
      "Training Decision Tree...\n",
      "Decision Tree\n",
      "----------------------------------------\n",
      "Accuracy  : 0.6998\n",
      "Precision : 0.6496\n",
      "Recall    : 0.6900\n",
      "F1-score  : 0.6529\n",
      "\n",
      "ROC-AUC   : 0.7471\n",
      "Training time: 6.14s\n",
      "================================================================================\n",
      "\n",
      "Training SVM (RBF)...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "results = {}\n",
    "training_times = {}\n",
    "\n",
    "print(\"Starting model training and evaluation...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Evaluate model (fit + predict + metrics)\n",
    "    metrics = evaluate_model(name, model, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    training_times[name] = training_time\n",
    "    \n",
    "    # Calculate ROC-AUC (requires probability estimates)\n",
    "    try:\n",
    "        if hasattr(model.named_steps['clf'], 'predict_proba'):\n",
    "            y_proba = model.predict_proba(X_test)[:, 1]\n",
    "            roc_auc = roc_auc_score(y_test, y_proba)\n",
    "            metrics['roc_auc'] = float(roc_auc)\n",
    "            print(f\"ROC-AUC   : {roc_auc:.4f}\")\n",
    "    except:\n",
    "        metrics['roc_auc'] = None\n",
    "    \n",
    "    metrics['training_time'] = training_time\n",
    "    results[name] = metrics\n",
    "    \n",
    "    print(f\"Training time: {training_time:.2f}s\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nAll models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.sort_values('f1', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\" * 100)\n",
    "print(results_df.to_string())\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "\n",
    "# Highlight best model\n",
    "best_model = results_df.index[0]\n",
    "best_f1 = results_df.loc[best_model, 'f1']\n",
    "print(f\"\\nBest Model (by F1-score): {best_model}\")\n",
    "print(f\"F1-score: {best_f1:.4f}\")\n",
    "print(f\"\\nBaseline (Jim's LogReg): F1 = 0.6584\")\n",
    "print(f\"Improvement: {(best_f1 - 0.6584) / 0.6584 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "titles = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Sort by metric value\n",
    "    sorted_results = results_df.sort_values(metric, ascending=True)\n",
    "    \n",
    "    # Plot\n",
    "    y_pos = range(len(sorted_results))\n",
    "    colors = ['green' if x > 0.7 else 'orange' if x > 0.6 else 'red' \n",
    "              for x in sorted_results[metric]]\n",
    "    \n",
    "    ax.barh(y_pos, sorted_results[metric], color=colors, edgecolor='black')\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(sorted_results.index)\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_title(f'{title} Comparison', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    ax.set_xlim(0, 1)\n",
    "    \n",
    "    # Add baseline line (Jim's LogReg)\n",
    "    if metric == 'accuracy':\n",
    "        baseline = 0.7071\n",
    "    elif metric == 'precision':\n",
    "        baseline = 0.6536\n",
    "    elif metric == 'recall':\n",
    "        baseline = 0.6932\n",
    "    elif metric == 'f1':\n",
    "        baseline = 0.6584\n",
    "    \n",
    "    ax.axvline(baseline, color='blue', linestyle='--', linewidth=2, \n",
    "               label=f\"Baseline: {baseline:.4f}\")\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, val in enumerate(sorted_results[metric]):\n",
    "        ax.text(val + 0.01, i, f'{val:.4f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "sorted_times = results_df.sort_values('training_time', ascending=True)\n",
    "y_pos = range(len(sorted_times))\n",
    "\n",
    "ax.barh(y_pos, sorted_times['training_time'], edgecolor='black', alpha=0.7)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(sorted_times.index)\n",
    "ax.set_xlabel('Training Time (seconds)')\n",
    "ax.set_title('Training Time Comparison', fontweight='bold', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, val in enumerate(sorted_times['training_time']):\n",
    "    ax.text(val + 0.5, i, f'{val:.2f}s', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC comparison (for models that support it)\n",
    "roc_auc_results = results_df[results_df['roc_auc'].notna()].sort_values('roc_auc', ascending=True)\n",
    "\n",
    "if len(roc_auc_results) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    y_pos = range(len(roc_auc_results))\n",
    "    colors = ['green' if x > 0.75 else 'orange' if x > 0.7 else 'red' \n",
    "              for x in roc_auc_results['roc_auc']]\n",
    "    \n",
    "    ax.barh(y_pos, roc_auc_results['roc_auc'], color=colors, edgecolor='black')\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(roc_auc_results.index)\n",
    "    ax.set_xlabel('ROC-AUC Score')\n",
    "    ax.set_title('ROC-AUC Comparison', fontweight='bold', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    ax.set_xlim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, val in enumerate(roc_auc_results['roc_auc']):\n",
    "        ax.text(val + 0.01, i, f'{val:.4f}', va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No models with ROC-AUC scores available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model by F1-score\n",
    "best_model_name = results_df.index[0]\n",
    "best_model_pipeline = models[best_model_name]\n",
    "\n",
    "# Get predictions\n",
    "y_pred = best_model_pipeline.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=True,\n",
    "            xticklabels=['Not Prolonged (0)', 'Prolonged (1)'],\n",
    "            yticklabels=['Not Prolonged (0)', 'Prolonged (1)'])\n",
    "ax.set_xlabel('Predicted Label', fontweight='bold')\n",
    "ax.set_ylabel('True Label', fontweight='bold')\n",
    "ax.set_title(f'Confusion Matrix - {best_model_name}', fontweight='bold', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed metrics\n",
    "print(f\"\\nConfusion Matrix for {best_model_name}:\")\n",
    "print(f\"True Negatives (TN):  {cm[0, 0]:,}\")\n",
    "print(f\"False Positives (FP): {cm[0, 1]:,}\")\n",
    "print(f\"False Negatives (FN): {cm[1, 0]:,}\")\n",
    "print(f\"True Positives (TP):  {cm[1, 1]:,}\")\n",
    "print(f\"\\nPer-class accuracy:\")\n",
    "print(f\"  Class 0 (Not Prolonged): {cm[0, 0] / cm[0].sum():.4f}\")\n",
    "print(f\"  Class 1 (Prolonged):     {cm[1, 1] / cm[1].sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n1. BEST PERFORMING MODEL\")\n",
    "print(f\"   Model: {best_model_name}\")\n",
    "print(f\"   F1-Score: {results_df.loc[best_model_name, 'f1']:.4f}\")\n",
    "print(f\"   Accuracy: {results_df.loc[best_model_name, 'accuracy']:.4f}\")\n",
    "print(f\"   Training Time: {results_df.loc[best_model_name, 'training_time']:.2f}s\")\n",
    "\n",
    "print(f\"\\n2. SGDClassifier PERFORMANCE (for Partial Fitting)\")\n",
    "sgd_f1 = results_df.loc['SGDClassifier', 'f1']\n",
    "print(f\"   F1-Score: {sgd_f1:.4f}\")\n",
    "print(f\"   Accuracy: {results_df.loc['SGDClassifier', 'accuracy']:.4f}\")\n",
    "print(f\"   vs Best Model: {(sgd_f1 - results_df.loc[best_model_name, 'f1']):.4f} difference\")\n",
    "print(f\"   Note: SGDClassifier supports partial_fit() for federated learning\")\n",
    "\n",
    "print(f\"\\n3. BASELINE COMPARISON (Jim's Logistic Regression)\")\n",
    "baseline_f1 = 0.6584\n",
    "improvement = (best_f1 - baseline_f1) / baseline_f1 * 100\n",
    "print(f\"   Baseline F1: {baseline_f1:.4f}\")\n",
    "print(f\"   Best F1: {best_f1:.4f}\")\n",
    "print(f\"   Improvement: {improvement:+.2f}%\")\n",
    "\n",
    "print(f\"\\n4. FASTEST MODEL\")\n",
    "fastest_model = results_df.sort_values('training_time').index[0]\n",
    "print(f\"   Model: {fastest_model}\")\n",
    "print(f\"   Training Time: {results_df.loc[fastest_model, 'training_time']:.2f}s\")\n",
    "print(f\"   F1-Score: {results_df.loc[fastest_model, 'f1']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\")\n",
    "print(\"1. Hyperparameter tuning for top-performing models\")\n",
    "print(\"2. Implement partial fitting with SGDClassifier to simulate federated learning\")\n",
    "print(\"3. Feature importance analysis for tree-based models\")\n",
    "print(\"4. Cross-validation for more robust performance estimates\")\n",
    "print(\"5. Test with different preprocessing strategies (e.g., different imputation methods)\")\n",
    "print(\"6. Address class imbalance with SMOTE or other sampling techniques\")\n",
    "print(\"\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results for Future Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "output_path = Path(\"../../results/model_comparison_results.csv\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "results_df.to_csv(output_path)\n",
    "\n",
    "print(f\"Results saved to: {output_path}\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  - {len(models)} models evaluated\")\n",
    "print(f\"  - Best F1-score: {best_f1:.4f} ({best_model_name})\")\n",
    "print(f\"  - All results saved for future analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
